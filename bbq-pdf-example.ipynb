{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ba848f-f652-4d8b-95c9-2c222b0c1507",
   "metadata": {},
   "source": [
    "## Setup for Elasticsearch\n",
    "This demo is intended for use with the (`start_local`)[] docker image that provides a perfect instance of Elastic's full offerings to get started.\n",
    "\n",
    "Run this command from a terminal to get started:\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://elastic.co/start-local | sh\n",
    "```\n",
    "\n",
    "Note that you must have Docker Desktop or similar installed and running first.\n",
    "\n",
    "After some procedural code and installations, you'll see a few key piecees of information:\n",
    "- Elastic Password - this is to log into Kibana \n",
    "- Elastic API KEY - this is unique to your docker instance.\n",
    "\n",
    "You'll need these for the code below to work properly and to use the Kibaba visualizations and navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f821d91-d13f-4e11-bf6f-a0450cb54b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "import PyPDF2\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "client = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    "    # This is the Elastic API Key you see from running the start_local docker image\n",
    "    api_key=\"SnppODJKVUJEUXJXcENRZDFndTA6LTJQclVnbllUNE9LLTJiUHJqSnBRZw==\",\n",
    ")\n",
    "\n",
    "client.options(request_timeout=60*3)\n",
    "resp = client.ping()\n",
    "print(resp)\n",
    "\n",
    "def output(data):\n",
    "    json_data = json.dumps(data.body, indent=4)\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0de67e-cc3d-4b8c-a703-554fc4c985bb",
   "metadata": {},
   "source": [
    "## Simple (non-optimized) pdf to text converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae3f40-7ecd-4e9b-bb10-61cc146e9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        # Create a PdfReader object instead of PdfFileReader\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        # Initialize an empty string to store the text\n",
    "        text = ''\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca5ab7-ac01-4b68-b558-a5994290f7af",
   "metadata": {},
   "source": [
    "## Create an index to store our pdfs. \n",
    "Note that `content` will store the pdf text and `bbq_vector` will store an e5 dense vector. We have activated BBQ by setting the `index_options` `type` to `bbq_hnsw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79c327-e5b1-4c9c-9377-deca49d79a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbq_index = \"bbq-pdf-embeddings-text\"\n",
    "if not client.indices.exists(index=bbq_index):\n",
    "    resp = client.indices.create(\n",
    "        index=bbq_index,\n",
    "        mappings={\n",
    "            \"properties\": {\n",
    "                \"content\": {\n",
    "                    \"type\": \"text\",\n",
    "                },\n",
    "                \"bbq_vector\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 384,\n",
    "                    \"index_options\": {\n",
    "                        \"type\": \"bbq_hnsw\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    output(resp)\n",
    "\n",
    "else:\n",
    "    print(f'The index {bbq_index} already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947240f-70f0-42a3-a2d9-2565e39456d4",
   "metadata": {},
   "source": [
    "## Create an Inference Endpoint\n",
    "This will create an inference endpoint named `bbq-e5-model` that will allow us to vectorize our pdf file contents. You'll want to download this in Kibana under [Trained Models](http://localhost:5601/app/ml/trained_models) if running for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a14a6-c069-445e-b337-e5e64a24d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_id = \"bbq-e5-model\"\n",
    "\n",
    "try:\n",
    "    client.inference.delete(\n",
    "        task_type=\"text_embedding\",\n",
    "        inference_id=inference_id,\n",
    "        force=True\n",
    "    )\n",
    "finally:\n",
    "    print(f'{inference_id} doesn\\'t exist, creating it now.')\n",
    "    resp = client.inference.put(\n",
    "        task_type=\"text_embedding\",\n",
    "        inference_id=inference_id,\n",
    "        inference_config={\n",
    "            \"service\": \"elasticsearch\",\n",
    "            \"service_settings\": {\n",
    "                \"num_allocations\": 1,\n",
    "                \"num_threads\": 1,\n",
    "                \"model_id\": \".multilingual-e5-small\"\n",
    "            },\n",
    "            \"chunking_settings\": {\n",
    "                \"strategy\": \"sentence\",\n",
    "                \"max_chunk_size\": 25,\n",
    "                \"sentence_overlap\": 1\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    output(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94952c37-f776-4fce-a58d-6568cb5f9737",
   "metadata": {},
   "source": [
    "## Create an ingest pipeline\n",
    "This ensures that all text within the `content` field will be copied to the `bbq_vector` field and vectorized appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e9eb5-517a-406d-bda9-0666e9ae1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resp = client.ingest.put_pipeline(\n",
    "    id=\"my_bbq_inference_pipeline\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": \"bbq-e5-model\",\n",
    "                \"input_output\": [\n",
    "                    {\n",
    "                        \"input_field\": \"content\",\n",
    "                        \"output_field\": \"bbq_vector\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "output(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc99ca-7f51-4906-b679-8d42c7775101",
   "metadata": {},
   "source": [
    "## Simulate the inference pipeline in action\n",
    "Running this will perform a dry run of the ingest pipeline. No data will actually be saved, but we will be able to observe our efforts thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc912776-8b48-4f59-8e8b-c2fd3d600b8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = pdf_to_text(\"WAC-SMALL/WAC 245.pdf\")\n",
    "\n",
    "resp = client.ingest.simulate(\n",
    "    id=\"my_bbq_inference_pipeline\",\n",
    "    docs=[\n",
    "        {\n",
    "            \"_source\": {\n",
    "                \"content\": text\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "output(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69168c-1c1d-4471-bddb-fbdd3462ec1c",
   "metadata": {},
   "source": [
    "## Bulk helper function\n",
    "This function creates an array of documents to insert into Elasticsearch for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644738c-4513-4926-82df-45da6dacb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate actions for the bulk API\n",
    "def generate_actions(pdf_dir, index):\n",
    "    try:\n",
    "        for filename in os.listdir(pdf_dir):\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                file_path = os.path.join(pdf_dir, filename)\n",
    "                print(f'------------------------------\\nProcessing {file_path}...')\n",
    "                text = pdf_to_text(file_path)\n",
    "                print(len(text))\n",
    "                yield {\n",
    "                    \"_index\": index,\n",
    "                    \"_source\": {\n",
    "                        \"content\": text\n",
    "                    }\n",
    "                }\n",
    "    except:\n",
    "        print('There was an error')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aced95c-ed8c-4e56-baf2-6a19134a34e9",
   "metadata": {},
   "source": [
    "## Index the docs!\n",
    "This will index a folder of pdf files in the `WAC-SMALL` pdf folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3853e9b-0f50-4454-a321-558306d89691",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = generate_actions(\"WAC-SMALL/\", \"bbq-pdf-embeddings-text\")\n",
    "\n",
    "helpers.bulk(client, actions , pipeline=\"my_bbq_inference_pipeline\", request_timeout=60*3)\n",
    "\n",
    "print(\"Finished indexing PDF documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942c9fb-1daf-440b-9e54-3c074d463a36",
   "metadata": {},
   "source": [
    "## Delete All Documents in the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb8451-7ee5-4c17-ac48-e45bb508c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.delete_by_query(\n",
    "    index=\"bbq-pdf-embeddings-text\",\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        }\n",
    "    },\n",
    "    refresh=True  # optional: forces immediate visibility\n",
    ")\n",
    "\n",
    "print(f\"✅ Deleted {resp['deleted']} documents from bbq-pdf-embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
